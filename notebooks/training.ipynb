{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/LuthandoMaqondo/magvit2-pytorch/blob/luthando-contribution/notebooks/training.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount the Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import requests\n",
    "import torch\n",
    "import wandb\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    WORKING_DIR = '.'\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    WORKING_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
    "    drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START The MAGVIT-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install magvit2-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 training samples found at /Users/luthandomaqondo/.cache/datasets/Appimate/train\n",
      "training with dataset of 9 samples and validating with randomly splitted 1 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16413) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(16414) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "python(16419) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(16420) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluthando957\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "python(16430) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(16431) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3e0aaef3d3406582ba928ab5d12b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011145238888841252, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/luthandomaqondo/Development/Python/magvit2-pytorch/notebooks/wandb/run-20240204_144247-imb02dm6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luthando957/magvit2/runs/imb02dm6' target=\"_blank\">avid-sponge-1</a></strong> to <a href='https://wandb.ai/luthando957/magvit2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luthando957/magvit2' target=\"_blank\">https://wandb.ai/luthando957/magvit2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luthando957/magvit2/runs/imb02dm6' target=\"_blank\">https://wandb.ai/luthando957/magvit2/runs/imb02dm6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Conv3D is not supported on MPS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m\n\u001b[1;32m     30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m VideoTokenizerTrainer(\n\u001b[1;32m     31\u001b[0m     tokenizer,\n\u001b[1;32m     32\u001b[0m     dataset_folder \u001b[38;5;241m=\u001b[39m dataset_folder,     \u001b[38;5;66;03m# folder of either videos or images, depending on setting below\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     use_wandb_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtrackers(project_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmagvit2\u001b[39m\u001b[38;5;124m'\u001b[39m, run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/magvit2_pytorch/trainer.py:513\u001b[0m, in \u001b[0;36mVideoTokenizerTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_steps:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(dl_iter)\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_main \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_every_step):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/magvit2_pytorch/trainer.py:347\u001b[0m, in \u001b[0;36mVideoTokenizerTrainer.train_step\u001b[0;34m(self, dl_iter)\u001b[0m\n\u001b[1;32m    344\u001b[0m     data, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dl_iter)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast(), context():\n\u001b[0;32m--> 347\u001b[0m         loss, loss_breakdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    348\u001b[0m             data,\n\u001b[1;32m    349\u001b[0m             return_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    350\u001b[0m             adversarial_loss_weight \u001b[38;5;241m=\u001b[39m adversarial_loss_weight,\n\u001b[1;32m    351\u001b[0m             multiscale_adversarial_loss_weight \u001b[38;5;241m=\u001b[39m multiscale_adversarial_loss_weight\n\u001b[1;32m    352\u001b[0m         )\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    357\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    358\u001b[0m     recon_loss \u001b[38;5;241m=\u001b[39m loss_breakdown\u001b[38;5;241m.\u001b[39mrecon_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    359\u001b[0m     perceptual_loss \u001b[38;5;241m=\u001b[39m loss_breakdown\u001b[38;5;241m.\u001b[39mperceptual_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    360\u001b[0m     adversarial_gen_loss \u001b[38;5;241m=\u001b[39m loss_breakdown\u001b[38;5;241m.\u001b[39madversarial_gen_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    361\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pytorch_custom_utils/accelerate_utils.py:84\u001b[0m, in \u001b[0;36mForwardingWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     83\u001b[0m   call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<@beartype(magvit2_pytorch.magvit2_pytorch.VideoTokenizer.forward) at 0x1474d4cc0>:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(__beartype_object_5126010976, __beartype_get_violation, __beartype_conf, __beartype_object_5461077952, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/magvit2_pytorch/magvit2_pytorch.py:1688\u001b[0m, in \u001b[0;36mVideoTokenizer.forward\u001b[0;34m(self, video_or_images, cond, return_loss, return_codes, return_recon, return_discr_loss, return_recon_loss_only, apply_gradient_penalty, video_contains_first_frame, adversarial_loss_weight, multiscale_adversarial_loss_weight)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m divisible_by(frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(video_contains_first_frame), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_downsample_factor), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of frames \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minus the first frame (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframes\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mint\u001b[39m(video_contains_first_frame)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be divisible by the total downsample factor across time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_downsample_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;66;03m# encoder\u001b[39;00m\n\u001b[0;32m-> 1688\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(video, cond \u001b[38;5;241m=\u001b[39m cond, video_contains_first_frame \u001b[38;5;241m=\u001b[39m video_contains_first_frame)\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# lookup free quantization\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_fsq:\n",
      "File \u001b[0;32m<@beartype(magvit2_pytorch.magvit2_pytorch.VideoTokenizer.encode) at 0x1474d4720>:55\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(__beartype_object_5126010976, __beartype_get_violation, __beartype_conf, __beartype_object_5461077952, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/magvit2_pytorch/magvit2_pytorch.py:1550\u001b[0m, in \u001b[0;36mVideoTokenizer.encode\u001b[0;34m(self, video, quantize, cond, video_contains_first_frame)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     pad, first_frame, video \u001b[38;5;241m=\u001b[39m unpack(video, video_packed_shape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c * h w\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1548\u001b[0m     first_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_in_first_frame(first_frame)\n\u001b[0;32m-> 1550\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_in(video)\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encode_first_frame_separately:\n\u001b[1;32m   1553\u001b[0m     video, _ \u001b[38;5;241m=\u001b[39m pack([first_frame, video], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c * h w\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/magvit2_pytorch/magvit2_pytorch.py:925\u001b[0m, in \u001b[0;36mCausalConv3d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    922\u001b[0m pad_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_mode \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_pad \u001b[38;5;241m<\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    924\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_causal_padding, mode \u001b[38;5;241m=\u001b[39m pad_mode)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    604\u001b[0m     )\n\u001b[0;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    607\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Conv3D is not supported on MPS"
     ]
    }
   ],
   "source": [
    "from magvit2_pytorch import (\n",
    "    VideoTokenizer,\n",
    "    VideoTokenizerTrainer\n",
    ")\n",
    "\n",
    "tokenizer = VideoTokenizer(\n",
    "    image_size = 128,\n",
    "    init_dim = 64,\n",
    "    max_dim = 512,\n",
    "    codebook_size = 1024,\n",
    "    layers = (\n",
    "        'residual',\n",
    "        'compress_space',\n",
    "        ('consecutive_residual', 2),\n",
    "        'compress_space',\n",
    "        ('consecutive_residual', 2),\n",
    "        'linear_attend_space',\n",
    "        'compress_space',\n",
    "        ('consecutive_residual', 2),\n",
    "        'attend_space',\n",
    "        'compress_time',\n",
    "        ('consecutive_residual', 2),\n",
    "        'compress_time',\n",
    "        ('consecutive_residual', 2),\n",
    "        'attend_time',\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset_folder = os.path.expanduser(f\"{WORKING_DIR}/datasets/Appimate/train\") if IN_COLAB else os.path.expanduser(f\"~/.cache/datasets/Appimate/train\")\n",
    "trainer = VideoTokenizerTrainer(\n",
    "    tokenizer,\n",
    "    dataset_folder = dataset_folder,     # folder of either videos or images, depending on setting below\n",
    "    dataset_type = 'videos',                        # 'videos' or 'images', prior papers have shown pretraining on images to be effective for video synthesis\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,\n",
    "    learning_rate = 2e-5,\n",
    "    num_train_steps = 1_000,\n",
    "    use_wandb_tracking = True,\n",
    ")\n",
    "with trainer.trackers(project_name = 'magvit2', run_name = 'baseline'):\n",
    "    if wandb.api.api_key is None:\n",
    "        key = getpass(\"Access key: \")\n",
    "        wandb.login(key=key, relogin=True)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after a lot of training ...\n",
    "# # can use the EMA of the tokenizer\n",
    "# ema_tokenizer = trainer.ema_tokenizer\n",
    "\n",
    "# # mock video\n",
    "# video = torch.randn(1, 3, 17, 128, 128)\n",
    "\n",
    "# # tokenizing video to discrete codes\n",
    "# codes = ema_tokenizer.tokenize(video) # (1, 9, 16, 16) <- in this example, time downsampled by 4x and space downsampled by 8x. flatten token ids for (non)-autoregressive training\n",
    "\n",
    "# # sanity check\n",
    "# decoded_video = ema_tokenizer.decode_from_code_indices(codes)\n",
    "# assert torch.allclose(\n",
    "#     decoded_video,\n",
    "#     ema_tokenizer(video, return_recon = True)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
