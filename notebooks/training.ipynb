{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/LuthandoMaqondo/magvit2-pytorch/blob/luthando-contribution/notebooks/training.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount the Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import requests\n",
    "import torch\n",
    "import wandb\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    WORKING_DIR = '.'\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    WORKING_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
    "    drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START The MAGVIT-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install magvit2-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "from magvit2_pytorch import (\n",
    "    VideoTokenizer,\n",
    "    VideoTokenizerTrainer\n",
    ")\n",
    "\n",
    "tokenizer = VideoTokenizer(\n",
    "    image_size = 128,\n",
    "    init_dim = 64,\n",
    "    max_dim = 512,\n",
    "    codebook_size = 1024,\n",
    "    layers = (\n",
    "        'residual',\n",
    "        'compress_space',\n",
    "        ('consecutive_residual', 2),\n",
    "        'compress_space',\n",
    "        ('consecutive_residual', 2),\n",
    "        'linear_attend_space',\n",
    "        'compress_space',\n",
    "        ('consecutive_residual', 2),\n",
    "        'attend_space',\n",
    "        'compress_time',\n",
    "        ('consecutive_residual', 2),\n",
    "        'compress_time',\n",
    "        ('consecutive_residual', 2),\n",
    "        'attend_time',\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset_folder = os.path.expanduser(f\"{WORKING_DIR}/datasets/Appimate/train\") if IN_COLAB else os.path.expanduser(f\"~/.cache/datasets/Appimate/train\")\n",
    "trainer = VideoTokenizerTrainer(\n",
    "    tokenizer,\n",
    "    dataset_folder = dataset_folder,     # folder of either videos or images, depending on setting below\n",
    "    dataset_type = 'videos',                        # 'videos' or 'images', prior papers have shown pretraining on images to be effective for video synthesis\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,\n",
    "    learning_rate = 2e-5,\n",
    "    num_train_steps = 1_000,\n",
    "    use_wandb_tracking = True,\n",
    "    checkpoints_folder = f'{WORKING_DIR}/Supervised Learning/MAGVIT-v2/checkpoints',\n",
    "    results_folder = f'{WORKING_DIR}/Supervised Learning/MAGVIT-v2/results',\n",
    ")\n",
    "\n",
    "if wandb.api.api_key is None:\n",
    "    key = getpass(\"Access key: \")\n",
    "    wandb.login(key=key, relogin=True)\n",
    "with trainer.trackers(project_name = 'magvit2', run_name = 'baseline'):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after a lot of training ...\n",
    "# can use the EMA of the tokenizer\n",
    "ema_tokenizer = trainer.ema_tokenizer\n",
    "\n",
    "# mock video\n",
    "video = torch.randn(1, 3, 17, 128, 128)\n",
    "\n",
    "# tokenizing video to discrete codes\n",
    "codes = ema_tokenizer.tokenize(video) # (1, 9, 16, 16) <- in this example, time downsampled by 4x and space downsampled by 8x. flatten token ids for (non)-autoregressive training\n",
    "\n",
    "# sanity check\n",
    "decoded_video = ema_tokenizer.decode_from_code_indices(codes)\n",
    "assert torch.allclose(\n",
    "    decoded_video,\n",
    "    ema_tokenizer(video, return_recon = True)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
